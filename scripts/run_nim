#!/bin/bash
#vim:filetype=sh

# Run local instance of NIM.
API_KEY="$(pass show ngc/jaylyesk@inervi.co/ngc_dev)"
LOCAL_NIM_CACHE=~/.cache/nim
NIM_SERVER_PORT=8000
IMAGE=nvcr.io/nim/meta/llama-3.1-8b-instruct:latest

docker run -it \
	--rm \
	--gpus all \
	--shm-size=16GB \
	-v "$LOCAL_NIM_CACHE:/opt/nim/.cache" \
	-u "$(id -u)" \
	-p "$NIM_SERVER_PORT:$NIM_SERVER_PORT" \
	-e NGC_API_KEY=$API_KEY \
	-e CUDA_VISIBLE_DEVICES=0 \
	-e NIM_MAX_MODEL_LEN=8192 \
	-e NIM_ENABLE_KV_CACHE_REUSE=1 \
	-e NIM_SERVER_PORT=$NIM_SERVER_PORT \
	$IMAGE

# Sanity checks for model instance.
curl --request "GET" "http://localhost:8000/v1/models" \
	-H "Authorization: Bearer $API_KEY"

# Sanity checks for inference and warmup engine.
curl --request 'POST' "http://localhost:8000/v1/chat/completions" \
	-H "accept: application/json" \
	-H "Content-Type: application/json" \
	-H "Authorization: Bearer $API_KEY" \
	-d '{
    "model": "meta/llama-3.1-8b-instruct",
    "messages": [{"role":"user", "content":"Write a limerick about the wonders of GPU computing."}],
    "max_tokens": 32
	}'
